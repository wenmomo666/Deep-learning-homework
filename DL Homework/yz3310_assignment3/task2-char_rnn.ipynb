{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char-RNN implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. This network is first posted by Andrej Karpathy, you can find out about his original code on https://github.com/karpathy/char-rnn, the original code is written in *lua*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement Char-RNN using Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup\n",
    "In this part, we will read the data of our input text and process the text for later network training. There are two txt files in the data folder, for computing time consideration, we will use tinyshakespeare.txt here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open('data/tinyshakespeare.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "# and let's get a glance of what the text is\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n",
      "First Citi --- characters mapped to int --- > [18 47 56 57 58  1 15 47 58 47]\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "vocab_to_ind = {c: i for i, c in enumerate(vocab)}\n",
    "ind_to_vocab = dict(enumerate(vocab))\n",
    "text_as_int = np.array([vocab_to_ind[c] for c in text], dtype=np.int32)\n",
    "\n",
    "# We mapped the character as indexes from 0 to len(vocab)\n",
    "for char,_ in zip(vocab_to_ind, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), vocab_to_ind[char]))\n",
    "# Show how the first 10 characters from the text are mapped to integers\n",
    "print ('{} --- characters mapped to int --- > {}'.format(text[:10], text_as_int[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating batches\n",
    "Now that we have preprocessed our input data, we then need to partition our data, here we will use mini-batches to train our model, so how will we define our batches?\n",
    "\n",
    "Let's first clarify the concepts of batches:\n",
    "1. **batch_size**: Reviewing batches in CNN, if we have 100 samples and we set batch_size as 10, it means that we will send 10 samples to the network at one time. In RNN, batch_size have the same meaning, it defines how many samples we send to the network at one time.\n",
    "2. **sequence_length**: However, as for RNN, we store memory in our cells, we pass the information through cells, so we have this sequence_length concept, which also called 'steps', it defines how long a sequence is.\n",
    "\n",
    "From above two concepts, we here clarify the meaning of batch_size in RNN. Here, we define the number of sequences in a batch as N and the length of each sequence as M, so batch_size in RNN **still** represent the number of sequences in a batch but the data size of a batch is actually an array of size **[N, M]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish the get_batches() function below to generate mini-batches.\n",
    "\n",
    "Hint: this function defines a generator, use *yield*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(array, n_seqs, n_steps):\n",
    "    '''\n",
    "    Partition data array into mini-batches\n",
    "    input:\n",
    "    array: input data\n",
    "    n_seqs: number of sequences in a batch\n",
    "    n_steps: length of each sequence\n",
    "    output:\n",
    "    x: inputs\n",
    "    y: targets, which is x with one position shift\n",
    "       you can check the following figure to get the sence of what a target looks like\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(array) / batch_size)\n",
    "    # we only keep the full batches and ignore the left.\n",
    "    array = array[:batch_size * n_batches]\n",
    "    array = array.reshape((n_seqs, -1))\n",
    "    \n",
    "    # You should now create a loop to generate batches for inputs and targets\n",
    "    #############################################\n",
    "    #           TODO: YOUR CODE HERE            #\n",
    "    #############################################\n",
    "    while True:\n",
    "        np.random.shuffle(array)\n",
    "        for n in range(0,array.shape[1],n_steps):\n",
    "            x=array[:,n:n+n_steps]\n",
    "            y=np.zeros_like(x)\n",
    "            y[:,:-1],y[:,-1]=x[:,1:],x[:,0]\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[50 58 57  1 51 39 63  1 57 46]\n",
      " [57 47 53 52  1 53 44  1 56 43]\n",
      " [46 47 51  1 42 53 61 52  1 58]\n",
      " [ 1 43 52 43 51 63 11  0 37 43]\n",
      " [56 44 53 50 49  6  0 27 52  1]\n",
      " [ 1 40 43 43 52  1 57 47 52 41]\n",
      " [56 57  6  1 39 52 42  1 57 58]\n",
      " [18 47 56 57 58  1 15 47 58 47]\n",
      " [47 52  1 57 54 47 58 43  1 53]\n",
      " [52 58 43 42  1 60 47 56 58 59]]\n",
      "\n",
      "y\n",
      " [[58 57  1 51 39 63  1 57 46 50]\n",
      " [47 53 52  1 53 44  1 56 43 57]\n",
      " [47 51  1 42 53 61 52  1 58 46]\n",
      " [43 52 43 51 63 11  0 37 43  1]\n",
      " [44 53 50 49  6  0 27 52  1 56]\n",
      " [40 43 43 52  1 57 47 52 41  1]\n",
      " [57  6  1 39 52 42  1 57 58 56]\n",
      " [47 56 57 58  1 15 47 58 47 18]\n",
      " [52  1 57 54 47 58 43  1 53 47]\n",
      " [58 43 42  1 60 47 56 58 59 52]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(text_as_int, 10, 10)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Char-RNN model\n",
    "In this section, we will build our char-rnn model, it consists of input layer, rnn_cell layer, output layer, loss and optimizer, we will build them one by one.\n",
    "\n",
    "The goal is to predict new text after given prime word, so for our training data, we have to define inputs and targets, here is a figure that explains the structure of the Char-RNN network.\n",
    "\n",
    "![structure](img/charrnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish all TODOs in ecbm4040.CharRNN and the blanks in the following cells.\n",
    "\n",
    "**Note: The training process on following settings of parameters takes about 20 minutes on a GTX 1070 GPU, so you are suggested to use GCP for this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecbm4040.CharRNN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Set sampling as False(default), we can start training the network, we automatically save checkpoints in the folder /checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200  loss: 2.0194  0.1889 sec/batch\n",
      "step: 400  loss: 1.7171  0.1864 sec/batch\n",
      "step: 600  loss: 1.5774  0.1907 sec/batch\n",
      "step: 800  loss: 1.4624  0.1934 sec/batch\n",
      "step: 1000  loss: 1.5236  0.1900 sec/batch\n",
      "step: 1200  loss: 1.3919  0.1867 sec/batch\n",
      "step: 1400  loss: 1.3587  0.1975 sec/batch\n",
      "step: 1600  loss: 1.3317  0.1892 sec/batch\n",
      "step: 1800  loss: 1.3074  0.1925 sec/batch\n",
      "step: 2000  loss: 1.2914  0.1955 sec/batch\n",
      "step: 2200  loss: 1.2880  0.1961 sec/batch\n",
      "step: 2400  loss: 1.2556  0.1959 sec/batch\n",
      "step: 2600  loss: 1.2971  0.1892 sec/batch\n",
      "step: 2800  loss: 1.2453  0.1946 sec/batch\n",
      "step: 3000  loss: 1.1979  0.1891 sec/batch\n",
      "step: 3200  loss: 1.2134  0.1926 sec/batch\n",
      "step: 3400  loss: 1.2324  0.1902 sec/batch\n",
      "step: 3600  loss: 1.2242  0.1878 sec/batch\n",
      "step: 3800  loss: 1.1876  0.1914 sec/batch\n",
      "step: 4000  loss: 1.1696  0.1923 sec/batch\n",
      "step: 4200  loss: 1.2042  0.1956 sec/batch\n",
      "step: 4400  loss: 1.1697  0.1887 sec/batch\n",
      "step: 4600  loss: 1.1915  0.1883 sec/batch\n",
      "step: 4800  loss: 1.1811  0.1882 sec/batch\n",
      "step: 5000  loss: 1.1912  0.1960 sec/batch\n",
      "step: 5200  loss: 1.1576  0.1929 sec/batch\n",
      "step: 5400  loss: 1.1324  0.1882 sec/batch\n",
      "step: 5600  loss: 1.1603  0.1877 sec/batch\n",
      "step: 5800  loss: 1.1542  0.1873 sec/batch\n",
      "step: 6000  loss: 1.1517  0.1942 sec/batch\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'LSTM', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints_LSTM/i6000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints_LSTM/i2000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints_LSTM/i4000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints_LSTM/i6000_l256.ckpt\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints_LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Set the sampling as True and we can generate new characters one by one. We can use our saved checkpoints to see how the network learned gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i6000_l256.ckpt\n",
      "LORD TYRANUS:\n",
      "So; they then, will thine on your head is troth,\n",
      "And well I have all my call that be made as long.\n",
      "\n",
      "PARIS:\n",
      "To the mother, what are your mother?\n",
      "\n",
      "Secont Servingman:\n",
      "What swear, I will not so stoop on wife?\n",
      "\n",
      "PETER:\n",
      "I have sorrow stain the state to tranch thou art.\n",
      "\n",
      "LUCIO:\n",
      "I will not send me for their servant of your silling and they\n",
      "are an indouching of to trump,\n",
      "And then I must altogety to thine entingive;\n",
      "I must be so, then here come forth to thee.\n",
      "\n",
      "KING RICHARD III:\n",
      "And would you have the power of him and his\n",
      "meaning.\n",
      "\n",
      "CAPULET:\n",
      "I am a sight of me out of your beauty,\n",
      "I will be to that strange, tell me that to-day,\n",
      "Which the south of his cunnor with the world,\n",
      "The pardon than strive with him to step a supble, which\n",
      "such little broughts that save and marry wings, thou hast made\n",
      "an one where when thy son, thou hast but bid, a break\n",
      "An end in his soul fortune will be pride:\n",
      "As well as thou and this still be too married,\n",
      "And made undolators, marcesty, the world\n",
      "A black all alone, a\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps,'LSTM', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_LSTM/i4000_l256.ckpt\n",
      "LORD AUMILO:\n",
      "A cundiss that, a word. A with yonder things\n",
      "To the corn, for this sons hath to supplod'd.\n",
      "\n",
      "GLOUCESTER:\n",
      "I am a greatire of my beauty.\n",
      "\n",
      "GREMIO:\n",
      "It is the treason of the case of her\n",
      "Thou straight, and that I can tell your brief to him.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Now, to him for his commonwealth the senators,\n",
      "Which he having tamed the wars of state warm man,\n",
      "When you have barrable strive to their horses,\n",
      "With his assisting battle strokes with women.\n",
      "\n",
      "KING RICHARD III:\n",
      "With her shin not, my gallant I would\n",
      "When I were short with a crown of me as\n",
      "But my true ladys' seas, should she his son:\n",
      "Thou hast spoke to this dead, why, then you will be,\n",
      "And being the point of thine a true our war;\n",
      "Which with and throne shall have this thought of hang.\n",
      "\n",
      "GLEUCHER:\n",
      "A penitent hole, and thou hast my state,\n",
      "Thou whight not how? and this to my be arm,\n",
      "That's my the postern of my thoughts.\n",
      "\n",
      "BUCKINGHAM:\n",
      "The gates of the shall--this deceit,\n",
      "I had that thought it seems too for his part.\n",
      "But what he conspitest to that\n"
     ]
    }
   ],
   "source": [
    "# choose a checkpoint other than the final one and see the results. It could be nasty, don't worry!\n",
    "#############################################\n",
    "#           TODO: YOUR CODE HERE            #\n",
    "#############################################\n",
    "checkpoint = 'checkpoints_LSTM/i4000_l256.ckpt'\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change another type of RNN cell\n",
    "We are using LSTM cell as the original work, but GRU cell is getting more popular today, let's chage the cell in rnn_cell layer to GRU cell and see how it performs. Your number of step should be the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to change your saved checkpoints' name or they will rewrite the LSTM results that you have already saved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200  loss: 1.9665  0.1746 sec/batch\n",
      "step: 400  loss: 1.6980  0.1738 sec/batch\n",
      "step: 600  loss: 1.5670  0.1748 sec/batch\n",
      "step: 800  loss: 1.4581  0.1760 sec/batch\n",
      "step: 1000  loss: 1.5451  0.1734 sec/batch\n",
      "step: 1200  loss: 1.3833  0.1834 sec/batch\n",
      "step: 1400  loss: 1.3690  0.1731 sec/batch\n",
      "step: 1600  loss: 1.3436  0.1729 sec/batch\n",
      "step: 1800  loss: 1.3182  0.1842 sec/batch\n",
      "step: 2000  loss: 1.3090  0.1771 sec/batch\n",
      "step: 2200  loss: 1.3000  0.1788 sec/batch\n",
      "step: 2400  loss: 1.2858  0.1761 sec/batch\n",
      "step: 2600  loss: 1.3043  0.1755 sec/batch\n",
      "step: 2800  loss: 1.2655  0.1789 sec/batch\n",
      "step: 3000  loss: 1.2446  0.1820 sec/batch\n",
      "step: 3200  loss: 1.2382  0.1844 sec/batch\n",
      "step: 3400  loss: 1.2618  0.1759 sec/batch\n",
      "step: 3600  loss: 1.2537  0.1771 sec/batch\n",
      "step: 3800  loss: 1.2227  0.1802 sec/batch\n",
      "step: 4000  loss: 1.2143  0.1762 sec/batch\n",
      "step: 4200  loss: 1.2385  0.1786 sec/batch\n",
      "step: 4400  loss: 1.2033  0.1781 sec/batch\n",
      "step: 4600  loss: 1.2390  0.1747 sec/batch\n",
      "step: 4800  loss: 1.2245  0.1755 sec/batch\n",
      "step: 5000  loss: 1.2282  0.1840 sec/batch\n",
      "step: 5200  loss: 1.2093  0.1729 sec/batch\n",
      "step: 5400  loss: 1.1728  0.1738 sec/batch\n",
      "step: 5600  loss: 1.1965  0.1843 sec/batch\n",
      "step: 5800  loss: 1.1713  0.1789 sec/batch\n",
      "step: 6000  loss: 1.2039  0.1770 sec/batch\n"
     ]
    }
   ],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_GRU/i6000_l256.ckpt\n",
      "LORD ARCHIS:\n",
      "Whom with my servant doth the king's dear?\n",
      "\n",
      "BUCKINGHAM:\n",
      "What is the woes be true to hear the common man!\n",
      "\n",
      "KING EDWARD IV:\n",
      "The good, whose foul tongue seess his father.\n",
      "\n",
      "GLOUCESTER:\n",
      "I dare not long, and show me well as heavel,\n",
      "That he dreads such me thanks his shield to thee.\n",
      "What hath he should be consupt yates?\n",
      "\n",
      "LADY ANNE:\n",
      "Why, then, I think there.\n",
      "\n",
      "GLOUCESTER:\n",
      "Then the kump' day the duke of the day.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "But it is so, my larges all, but shadows meet.\n",
      "\n",
      "LADY ANNE:s true.\n",
      "\n",
      "KING RICHARD III:\n",
      "The soldiers of as he which the war\n",
      "Should bring me to my hand with things that made\n",
      "When then his hand the horrer than you went.\n",
      "\n",
      "GLOUCESTER:\n",
      "Where is to best croom out his father?\n",
      "\n",
      "KING EDWARD IV:\n",
      "And therefore I had shade the sea while then.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Ay, boy: it is the day, and there my tongue;\n",
      "Being another which days warm with them;\n",
      "And therefore give the sendous provered grave.\n",
      "\n",
      "KING EDWARD IV:\n",
      "I'll have my son I would ne'er came to thee;\n",
      "That whose deserves my state\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints_GRU')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Compare your result of two networks that you built and the reasons that caused the difference. (It is a qualitative comparison, it should be based on the specific model that you build.)\n",
    "2. Discuss the difference between LSTM cells and GRU cells, what are the pros and cons of using GRU cells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "1. GRU can be trained slightly faster than LSTM. (About 0.18 sec/batch vs 0.19 sec/batch, because LSTM has one more gate than GRU and thus need more tensor operation)  \n",
    "The performance of LSTM is a little bit better than GRU, which has more complete sentenses and words, because its gates design is more intricate.\n",
    "\n",
    "\n",
    "2. A GRU unlike an LSTM network does not have a cell state and has 2 gates instead of 3(forget, update, output). A gated recurrent unit (GRU) uses an update gate and a reset gate. The update gate decides on how much of information from the past should be let through and the reset gate decides on how much of information from the past should be discarded. Also, the GRU unit controls the flow of information like the LSTM unit, but without having to use a memory unit. It just exposes the full hidden content without any control.  \n",
    "*Pros:* GRU is computationally efficient and trained faster than an LSTM network, due to the reduction of gates.  \n",
    "*Cons:* GRU is not as good as LSTM in terms of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
